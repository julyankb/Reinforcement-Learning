{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map observation to set of coordinates\n",
    "def obs_to_state(obs):\n",
    "  arr = np.array(obs.layers['P'], dtype=np.float)\n",
    "  agent_position = np.argwhere(arr)[0]\n",
    "  a,b = agent_position[0], agent_position[1]\n",
    "  return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(game):\n",
    "    # Will keep a list of actions\n",
    "    action_list = []\n",
    "\n",
    "    # Init game\n",
    "    obs, reward, gamma = game.its_showtime()\n",
    "\n",
    "    while True:\n",
    "    # Get current state and select and action.\n",
    "    a,b = obs_to_state(obs)\n",
    "\n",
    "    # With probability epsilon\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "\n",
    "      if dynaplus:\n",
    "        # Dyna-Q+\n",
    "        unexplored_actions = []\n",
    "        explored_actions = []\n",
    "        for action in (0,1,2,3):\n",
    "          try:\n",
    "            endpoint = model[a][b][action]\n",
    "            explored_actions.append(action)\n",
    "          except:\n",
    "            unexplored_actions.append(action)\n",
    "\n",
    "        if len(explored_actions) == 4 or len(unexplored_actions) == 4:\n",
    "          action = np.random.randint(0,4)\n",
    "\n",
    "        else:\n",
    "          random_explored = np.random.choice(explored_actions)\n",
    "          random_unexplored = np.random.choice(unexplored_actions)\n",
    "          action = np.random.choice((random_explored, random_unexplored), p=(0.75, 0.25))\n",
    "\n",
    "      else:\n",
    "        action = np.random.randint(0,4)\n",
    "\n",
    "    # Otherwise\n",
    "    else:\n",
    "      action = np.argmax(q_table[a][b])\n",
    "\n",
    "    # Action results in a reward and subsequent state.\n",
    "    obs, reward, gamma = game.play(action)\n",
    "    a_,b_ = obs_to_state(obs)\n",
    "\n",
    "\n",
    "    # Keep a list of actions taken.\n",
    "    current_pos = (a_,b_)\n",
    "    #print(current_pos, last_pos)\n",
    "    if current_pos != last_pos:\n",
    "      action_list.append(action)\n",
    "      last_pos = current_pos\n",
    "    else:\n",
    "      action_list.append(-1)\n",
    "      last_pos = current_pos\n",
    "\n",
    "\n",
    "    # Update model with visited position.\n",
    "    '''\n",
    "    Each visited poisition has 4 possible actions, each of which have\n",
    "    a next state and a resulting reward. \n",
    "    '''\n",
    "    model[(a,b)] = {}\n",
    "    model[(a,b)][action] = ((a_,b_), reward)\n",
    "\n",
    "    # Update Q-table.\n",
    "    q_table[a][b][action] += alpha * (reward + gamma * np.max(q_table[a_][b_])\n",
    "                                      - q_table[a][b][action])\n",
    "\n",
    "    # Update reward achieved.\n",
    "    #reward_acheived += reward\n",
    "\n",
    "    # Update cumulative reward.\n",
    "    cumulative_rewards.append(cumulative_rewards[-1] + reward)\n",
    "\n",
    "    # Increment time steps.\n",
    "    steps_taken += 1\n",
    "\n",
    "\n",
    "    # ** Temp code, not sure if should break here.\n",
    "    if total_timesteps + steps_taken == switchpoint:\n",
    "        action_list.append(-3)\n",
    "        break\n",
    "\n",
    "    # End episode if player is on target 'G'.\n",
    "    if gamma == 0:\n",
    "      action_list.append(-2)\n",
    "      break\n",
    "\n",
    "    # If player not on G. Imagine scenarios based on memory, update Q table. \n",
    "    # Remember what you've done and what resulted from it, \n",
    "    # then reinforce that without taking any actions.\n",
    "    for i in range(n):\n",
    "      state = random.choice(list(model.keys()))\n",
    "      action = np.random.choice(list(model[state].keys()))\n",
    "      next_state, reward = model[state][action][0], model[state][action][1]\n",
    "\n",
    "      a,b = state\n",
    "      a_,b_ = next_state\n",
    "      q_table[a][b][action] += alpha * (reward + gamma * np.max(q_table[a_][b_])\n",
    "                                        - q_table[a][b][action])\n",
    "\n",
    "    return q_table, model, cumulative_rewards, steps_taken, action_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_matrix(policy):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
